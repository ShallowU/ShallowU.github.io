<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>lecture4-cs231n</title>
    <link href="/2024/02a889af7e.html"/>
    <url>/2024/02a889af7e.html</url>
    
    <content type="html"><![CDATA[<h1 id="1-卷积神经网络（CNNs-x2F-ConvNets）"><a href="#1-卷积神经网络（CNNs-x2F-ConvNets）" class="headerlink" title="1.卷积神经网络（CNNs &#x2F; ConvNets）"></a>1.卷积神经网络（CNNs &#x2F; ConvNets）</h1><p>相比于拉长input，使用卷积层更有利于保留数据的空间性质。<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20192116.png" alt="alt text"><br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20192102.png" alt="alt text"><br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20192258.png" alt="alt text"><br>卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。<br>回顾：常规神经网络。在上一章中，神经网络的输入是一个向量，然后在一系列的隐层中对它做变换。每个隐层都是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值</p><p>积神经网络主要由三种类型的层构成：卷积层，汇聚（Pooling）（downsampling）层和全连接层（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。</p><p>局部连接：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。</p><p>例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果（receptive field）感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3&#x3D;75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。</p><p>例2：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20&#x3D;180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。</p><p>上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）、activation map&#x3D;&#x3D;feature map、 kernal&#x3D;&#x3D;滤波器<br>首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。<br>其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。<br>在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个 零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。</p><p>一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：卷积层，汇聚（Pooling）层和全连接层（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。</p><p><em>网络结构例子：</em>这仅仅是个概述，下面会更详解的介绍细节。一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：</p><p>输入[32x32x3]存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道。<br>卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有神经元的输出。如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]。<br>ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的<br>作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。<br>汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。<br>全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE(456).png" alt="alt text"></p><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20192937.png" alt="使数据尺寸大小不变的zero-padding中p的取值"></p><h1 id="计算实例"><a href="#计算实例" class="headerlink" title="计算实例"></a>计算实例</h1><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20193046.png" alt="alt text"></p><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20193116.png" alt="alt text"></p><p>最近小尺寸kernal滤波器如1<em>1、2</em>2，也是十分常见和重要<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE(457).png" alt="alt text"></p><h1 id="conv特点"><a href="#conv特点" class="headerlink" title="conv特点"></a>conv特点</h1><p>·局部感受野<br>·权值共享<br>·下采样、池化<br>池化一般有max、或者average两种操作</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20202746.png" alt="alt text"></p><p>参考链接：<br><a href="https://iphysresearch.github.io/blog/post/dl_notes/cs231n/cs231n_convnet_notes/#25-%E6%8A%8A%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E8%BD%AC%E5%8C%96%E6%88%90%E5%8D%B7%E7%A7%AF%E5%B1%82">https://iphysresearch.github.io/blog/post/dl_notes/cs231n/cs231n_convnet_notes/#25-%E6%8A%8A%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E8%BD%AC%E5%8C%96%E6%88%90%E5%8D%B7%E7%A7%AF%E5%B1%82</a></p>]]></content>
    
    
    <categories>
      
      <category>deeplearnin</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs231n</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lecture3-cs231n</title>
    <link href="/2024/02a24ca667.html"/>
    <url>/2024/02a24ca667.html</url>
    
    <content type="html"><![CDATA[<p>“Neural Network” is a very broad term; these are more accurately called<br>“fully-connected networks” or sometimes “multi-layer perceptrons” (MLP)<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20105848.png" alt="Alt text"></p><blockquote><p>函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度</p></blockquote><h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p>反向传播是利用链式法则递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。<br>反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：1. 这个门的输出值，和2.其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</p><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>用于非线性数据经过层层激活函数可以变为线性<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20105756.png" alt="Alt text"></p><h1 id="用向量化操作计算梯度"><a href="#用向量化操作计算梯度" class="headerlink" title="用向量化操作计算梯度"></a>用向量化操作计算梯度</h1><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-03%20111444.png" alt="Alt text"><br>上述内容考虑的都是单个变量情况，但是所有概念都适用于矩阵和向量操作。然而，在操作的时候要注意关注维度和转置操作。<br>矩阵相乘的梯度：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作<br>提示：要分析维度！ 注意不需要去记忆dW和dX的表达，因为它们很容易通过维度推导出来。例如，权重的梯度dW的尺寸肯定和权重矩阵W的尺寸是一样的，而这又是由X和dD的矩阵乘法决定的（在上面的例子中X和W都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如，X的尺寸是[10x3]，dD的尺寸是[5x3]，如果你想要dW和W的尺寸是[5x10]，那就要dD.dot(X.T)。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。<br>讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。</p><p>参考文章：<br><a href="https://iphysresearch.github.io/blog/tag/cs231n/">https://iphysresearch.github.io/blog/tag/cs231n/</a></p>]]></content>
    
    
    <categories>
      
      <category>deeplearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs231n</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>01Communication-Efficient_Learning_of_Deep_Networks_from_Decentralized_Data</title>
    <link href="/2024/016631f8d2.html"/>
    <url>/2024/016631f8d2.html</url>
    
    <content type="html"><![CDATA[<p>论文标题：《Communication-Efficient Learning of Deep Networks from Decentralized Data》，发表于AISTATS 2017</p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><blockquote><p>We advocate an alternative that leaves the training data distributed on<br>the mobile devices, and learns a shared model by<br>aggregating locally-computed updates. We term<br>this decentralized approach Federated Learning</p></blockquote><blockquote><p>robust to the unbalanced and non-IID data distributions<br>Communication costs are the principal constraint, and<br>we show a reduction in required communication<br>rounds by 10–100× as compared to synchronized<br>stochastic gradient descent.</p></blockquote><p>现代移动设备能够接触到大量适合用于训练模型的数据，以进一步优化用户使用此类设备的体验，如语音识别、文字输入和图片选择等。但是这些数据一般是敏感的私人数据或是海量数据，不适合使用传统方法训练，因此作者提出一种基于分布式训练并共享聚合模型的训练方法，并命名为联邦学习(Fedrated Learning)</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><blockquote><p>amount of data, much of it private in nature</p></blockquote><blockquote><p>the sensitive nature of the data means there are risks and responsibilities to storing it in a centralized location.</p></blockquote><blockquote><p>only this update is communicated ,limiting the<br>attack surface to only the device, rather than the device and<br>the cloud.</p></blockquote><p>作者研究了一种允许用户们通过自己的高价值数据获得并共享更好的模型，但无需数据共享，并命名为联邦学习技术(因为模型学习目标是通过一个中央Server管理一个由参与模型学习的设备组成的松散联邦）。各个用户通过本地数据集训练本地模型，并将update发送到Server。整个联邦学习过程中，只有用户发送的update参与了communication，并且由于这些update只是针对当前的模型，应用后就没有任何必要存储他们了，在一定程度上确保了用户私人数据的安全。</p><p>联邦学习技术的重要优势在于分离了模型训练过程中对训练数据的直接访问，并通过限制攻击面极大程度上降低了隐私和安全风险</p><p>Contributions如下：<br>确认将分离式移动设备训练问题作为研究方向。<br>对可应用于联邦学习思想技术的算法选择。<br>引入了FederatedAveraging算法，该算法结合了每个客户端上的随机梯度下降和执行模型平均的服务器。通过实验显示了该算法对不平衡和非独立同分布数据的鲁棒性，并能减少在分布式数据上训练深度网络所需要的通信次数</p><blockquote><p>Our primary contributions are 1) the identification of the<br>problem of training on decentralized data from mobile devices as an important research direction; 2) the selection of<br>a straightforward and practical algorithm that can be applied<br>to this setting; and 3) an extensive empirical evaluation of<br>the proposed approach. More concretely, we introduce the<br>FederatedAveraging algorithm, which combines local stochastic gradient descent (SGD) on each client with<br>a server that performs model averaging. We perform extensive experiments on this algorithm, demonstrating it is<br>robust to unbalanced and non-IID data distributions, and<br>can reduce the rounds of communication needed to train a<br>deep network on decentralized data by orders of magnitude</p></blockquote><p><em>联邦学习的特点</em><br>数据大都来自于真实数据训练效果更加贴合实际；<br>数据高度敏感且相对于单个用户来说，数据量非常大；<br>对于有监督学习，可以通过与用户的互动轻松对数据进行标号。</p><p><em>联邦学习对于隐私性的保护</em><br>会进行通信的数据只有需要的更新，这保证了用户数据的安全；<br>更新数据不需要保存，一旦更新成功，更新数据将被丢失；<br>通过更新数据对原始数据的破解几乎不可能。</p><p><em>联邦学习与分布式学习有着几个显著的不同</em><br>数据分布非独立同分布：不同的用户有着不同的行为；<br>数据分布不平衡：指某些参与者的数据可能很多，而某些参与者数据可能很少；<br>大量的参与者：一个软件的用户可能非常多（例如某款输入法）；<br>受限的通信：参与者的信号可能非常差，甚至出现离线的情况；</p><p><em>联邦学习需要处理以下问题</em><br>各个参与方的数据可能会发生改变（例如删除、添加、编辑照片）；<br>参与方的数据分布非常复杂（不同的群体的手机使用情况差异可能会非常大</p><p><em>联邦学习与传统数据中心计算的不同</em><br>传统的数据中心计算，往往通信的消耗是相对较小的，计算的时间是相对较大的，联邦学习正好相反（用户可能只会在特定的时间才回进行上传，例如睡觉时，而由于用户的处理器往往不会太差，计算的耗费相对就会较小）</p><p>有两种方法能够缓解联邦学习中通信耗时的问题（核心都是让用户进行大量的计算，这样能减少通信时间的占比）：<br>增加并行度，让更多的参与者进行计算；<br>增加计算度，让一个参与者进行更多的计算。</p><p><em>相关工作介绍</em><br>传统的分布式学习考虑的是平衡的分布（各个参与方的计算量与各自的计算能力相匹配）和独立同分布（各方的数据是独立同分布的）；<br>传统的分布式学习只会进行一次集中更新，已经被证明，这样训练出来的模型在最坏情况下可能会比在单个参与方训练的模型效果差。</p><h1 id="The-FederatedAveraging-Algorithm"><a href="#The-FederatedAveraging-Algorithm" class="headerlink" title="The FederatedAveraging Algorithm"></a>The FederatedAveraging Algorithm</h1><p><img src="/img/paper/01Communication-Efficient_Learning_of_Deep_Networks_from_Decentralized_Data/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-01%20114852.png" alt="Alt text"><br>计算量由三个关键参数控制：C，在每轮上执行计算的客户端的比例；E，每个客户端在每轮上对其本地数据集进行的训练通过数；B，客户端更新所使用的本地小批量大小。我们写入B&#x3D;∞以指示将整个本地数据集作为单个小批处理，取B&#x3D;∞和E&#x3D;1，它正好对应于FedSGD。</p><p>B&#x3D;∞： 代表minibatch&#x3D;用户本地全部数据</p><p>B&#x3D;∞ &amp; E &#x3D; 1： FedAvg 等价于 FedSGD</p><p><img src="/img/paper/01Communication-Efficient_Learning_of_Deep_Networks_from_Decentralized_Data/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-01%20114921.png" alt="Alt text"></p><h1 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h1><p>查看<a href="https://blog.csdn.net/qq_45523675/article/details/127899784%EF%BC%8C%E4%BB%96%E5%86%99%E7%9A%84%E5%BE%88%E8%AF%A6%E7%BB%86%E5%BE%88%E4%B8%8D%E9%94%99">https://blog.csdn.net/qq_45523675/article/details/127899784，他写的很详细很不错</a></p><h1 id="Conclusions-and-Future-Work"><a href="#Conclusions-and-Future-Work" class="headerlink" title="Conclusions and Future Work"></a>Conclusions and Future Work</h1><blockquote><p>Our experiments show that federated learning can be made<br>practical, as FedAvg trains high-quality models using relatively few rounds of communication, as demonstrated by<br>results on a variety of model architectures: a multi-layer<br>perceptron, two different convolutional NNs, a two-layer<br>character LSTM, and a large-scale word-level LSTM.<br>While federated learning offers many practical privacy benefits, providing stronger guarantees via differential privacy [14, 13, 1], secure multi-party computation [18], or<br>their combination is an interesting direction for future work.<br>Note that both classes of techniques apply most naturally to<br>synchronous algorithms like FedAvg.</p></blockquote><p>各种模型架构（多层感知器，两种不同的卷积神经网络， 两层字符LSTM和大规模词级LSTM）的实验结果表明：当FedAvg使用相对较少的交流轮次来训练高质量的模型时，联邦学习是实际可行的。<br>尽管联合学习提供了许多实用的隐私优势，但通过差分隐私、多方安全计算或者它们的组合提供更强的隐私保证是未来工作的有趣方向</p><h1 id="参考文章链接"><a href="#参考文章链接" class="headerlink" title="参考文章链接"></a>参考文章链接</h1><p><a href="https://blog.csdn.net/qq_45523675/article/details/127899784">https://blog.csdn.net/qq_45523675/article/details/127899784</a><br><a href="https://zhuanlan.zhihu.com/p/656686796">https://zhuanlan.zhihu.com/p/656686796</a><br><a href="https://zhuanlan.zhihu.com/p/445458807">https://zhuanlan.zhihu.com/p/445458807</a></p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>federal_learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lecture2-cs231n</title>
    <link href="/2024/016ee6a6f9.html"/>
    <url>/2024/016ee6a6f9.html</url>
    
    <content type="html"><![CDATA[<h1 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h1><p>在上一节定义了从图像像素值到所属类别的评分（score function），该函数的参数是权重矩阵W。在函数中，数据(Xi,Yi)是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p><p>回到之前那张猫的图像分类例子，它有针对“猫”，“狗”，“船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用损失函数（Loss Function）（有时也叫代价函数Cost Function或目标函数Objective）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小</p><h1 id="多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss"><a href="#多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss" class="headerlink" title="多类支持向量机损失 Multiclass Support Vector Machine Loss"></a>多类支持向量机损失 Multiclass Support Vector Machine Loss</h1><p>损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值delta。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-26%20195352.png" alt="Alt text"></p><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-26%20195423.png" alt="Alt text"><br>在结束这一小节前，还必须提一下的属于是关于0的阀值：<br>函数，它常被称为折叶损失（hinge loss）。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是<br>，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-26%20195506.png" alt="Alt text"></p><h1 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h1><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-26%20200532.png" alt="Alt text"><br>本课中展示的多类SVM只是多种SVM公式中的一种。</p><h1 id="Softmax-Classifier-Multinomial-Logistic-Regression"><a href="#Softmax-Classifier-Multinomial-Logistic-Regression" class="headerlink" title="Softmax Classifier (Multinomial Logistic Regression)"></a>Softmax Classifier (Multinomial Logistic Regression)</h1><p> interpret raw classifier scores as probabilities<br> <img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-26%20200859.png"></p><p> <img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-26%20202137.png" alt="Alt text"><br> 针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量f（本节中是通过矩阵乘来实现）。不同之处在于对f中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的对数概率，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p><p> 还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。</p><p> 在实际使用中，SVM和Softmax经常是相似的：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（delta&#x3D;1）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没什么不同，只要满足超过边界值等于1，那么损失值就等于0。</p><p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p><h1 id="最优化-Optimization"><a href="#最优化-Optimization" class="headerlink" title="最优化 Optimization"></a>最优化 Optimization</h1><p>损失函数可以量化某个具体权重集W的质量。而最优化的目标就是找到能够最小化损失函数值的W 。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM 损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。<br>核心思路：迭代优化。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重W非常困难，甚至是不可能的（尤其当W中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集W取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的W开始，然后对其迭代取优，每次都让它的损失值变得更小一点。</p><p>我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。</p><p>蒙眼徒步者的比喻：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为W是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。<br>我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的梯度（gradient）。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。</p><p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数derivatives）。</p><p>梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作学习率）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p><p>小批量数据梯度下降（Mini-batch gradient descent）：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的 小批量（batches）数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。<br>梯度下降（Stochastic Gradient Descent 简称SGD），有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p><p>参考文章：<br><a href="https://iphysresearch.github.io/blog/tag/cs231n/">https://iphysresearch.github.io/blog/tag/cs231n/</a></p>]]></content>
    
    
    <categories>
      
      <category>deeplearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs231n</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lecture1-cs231n</title>
    <link href="/2024/01e069a11a.html"/>
    <url>/2024/01e069a11a.html</url>
    
    <content type="html"><![CDATA[<h1 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h1><p><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20114829.png" alt="Alt text"><br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20121234.png" alt="Alt text"><br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20121340.png" alt="Alt text"></p><h1 id="data-drive"><a href="#data-drive" class="headerlink" title="data_drive"></a>data_drive</h1><p>Image Classification<br>A Core Task in Computer Vision<br>Today:<br>● The image classification task</p><p>● Two basic data-driven approaches to image classification<br>○ K-nearest neighbor and linear classifier</p><p>&#x2F;*<br>An image is a tensor of integers<br>between [0, 255]:<br>e.g. 800 x 600 x 3<br>(3 channels RGB)<br>*&#x2F;</p><p>直接根绝分类对象写算法判断几乎不可能<br>干扰：<br>Illumination 光线<br>Background Clutter 背景混乱<br>Occlusion 只露出一部分<br>Deformation 会形变<br>······<br>no obvious way to hard-code the algorithm for<br>recognizing a cat, or other classes.</p><p>Machine Learning: Data-Driven Approach</p><ol><li>Collect a dataset of images and labels</li><li>Use Machine Learning algorithms to train a classifier</li><li>Evaluate the classifier on new images</li></ol><h1 id="Knn-K-Nearest-Neighbor-Classifier（最近邻分类器）"><a href="#Knn-K-Nearest-Neighbor-Classifier（最近邻分类器）" class="headerlink" title="Knn K Nearest Neighbor Classifier（最近邻分类器）"></a>Knn K Nearest Neighbor Classifier（最近邻分类器）</h1><p>KNN就是记忆存储大量得训练集，然后每次测试时候根据距离判断与测试图片最相似得K张照片，再根据投票选择最合适lable<br>（即矩阵差值最小的K个）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NearestNeighbor</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br><span class="hljs-keyword">pass</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; </span><br><span class="hljs-string">    这个地方的训练其实就是把所有的已有图片读取进来 -_-||</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-string">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># the nearest neighbor classifier simply remembers all the training data</span><br>    self.Xtr = X<br>    self.ytr = y<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br> <span class="hljs-string">&quot;&quot;&quot; </span><br><span class="hljs-string">    所谓的预测过程其实就是扫描所有训练集中的图片，计算距离，取最小的距离对应图片的类目</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-string">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span><br>    num_test = X.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment"># 这里要保证维度一致！</span><br>    <span class="hljs-comment"># lets make sure that the output type matches the input type</span><br>    Ypred = np.zeros(num_test, dtype = self.ytr.dtype) <span class="hljs-comment"># 一维元素都是0的array</span><br><br>        <span class="hljs-comment"># 把训练集扫一遍 -_-||</span><br><span class="hljs-comment"># loop over all test rows</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> xrange(num_test):  <br>            <span class="hljs-comment"># 注意这里的xrange仅适用于python2.x，range适用于python3.x</span><br>      <span class="hljs-comment"># find the nearest training image to the i&#x27;th test image</span><br>      <span class="hljs-comment"># using the L1 distance (sum of absolute value differences)</span><br>      <span class="hljs-comment"># 对训练集中每一张图片都与指定的一张测试图片，在对应元素位置上做差，</span><br>            <span class="hljs-comment"># 然后分别以每张图片为单位求和。</span><br>      distances = np.<span class="hljs-built_in">sum</span>(np.<span class="hljs-built_in">abs</span>(self.Xtr - X[i,:]), axis = <span class="hljs-number">1</span>)<span class="hljs-comment"># 一个5000元素的list</span><br>      <span class="hljs-comment"># 取最小distance图片的下标：</span><br>      min_index = np.argmin(distances) <span class="hljs-comment"># get the index with smallest distance</span><br>      Ypred[i] = self.ytr[min_index] <span class="hljs-comment"># predict the label of the nearest example</span><br><br>    <span class="hljs-keyword">return</span> Ypred<br></code></pre></td></tr></table></figure><p>两种距离：<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20160823.png" alt="Alt text"><br>超参数：得提前设置好<br>Hyperparameters<br>两个选择问题：<br>What is the best value of k to use?<br>What is the best distance to use?<br>These are hyperparameters: choices about<br>the algorithms themselves.<br>Very problem&#x2F;dataset-dependent.<br>Must try them all out and see what works best.<br>k-NN分类器需要设定k值，那么选择哪个k值最合适的呢？我们可以选择不同的距离函数，比如L1范数和L2范数等，那么选哪个好？还有不少选择我们甚至连考虑都没有考虑到（比如：点积）。所有这些选择，被称为超参数（hyperparameter）。在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般说来，这些超参数具体怎么设置或取值并不是显而易见的。</p><p>训练集、验证集、测试集<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20161231.png" alt="Alt text"><br>（Example Dataset: CIFAR10<br>10 classes<br>50,000 training images<br>10,000 testing images）</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-comment"># 假定已经有Xtr_rows, Ytr, Xte_rows, Yte了，其中Xtr_rows为50000*3072 矩阵</span><br><span class="hljs-comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span><br><span class="hljs-comment"># recall Xtr_rows is 50,000 x 3072 matrix</span><br>Xval_rows = Xtr_rows[:1000, :] <span class="hljs-comment"># take first 1000 for validation 构建前1000个图为交叉验证集</span><br>Yval = Ytr[:1000]<br>Xtr_rows = Xtr_rows[1000:, :] <span class="hljs-comment"># keep last 49,000 for train 保留其余49000个图为训练集</span><br>Ytr = Ytr[1000:]<br><br><span class="hljs-comment"># 设置一些k值，用于试验</span><br><span class="hljs-comment"># find hyperparameters that work best on the validation set</span><br>validation_accuracies = []<br>for k in [1, 3, 5, 10, 20, 50, 100]:<br><br>    <span class="hljs-comment"># 初始化对象</span><br>    <span class="hljs-comment"># use a particular value of k and evaluation on validation data</span><br>    nn = NearestNeighbor()<br>nn.train(Xtr_rows, Ytr)<br>    <span class="hljs-comment"># 修改一下predict函数，接受 k 作为参数</span><br>  <span class="hljs-comment"># here we assume a modified NearestNeighbor class that can take a k as input</span><br>  Yval_predict = nn.predict(Xval_rows, k = k)<br>  acc = np.mean(Yval_predict == Yval)<br>  print &#x27;accuracy: %f&#x27; % (acc,)<br><br>    <span class="hljs-comment"># 输出结果</span><br>  <span class="hljs-comment"># keep track of what works on the validation set</span><br>  validation_accuracies.append((k, acc)) <span class="hljs-comment"># 元组形式append在列表里</span><br><br></code></pre></td></tr></table></figure><p>K-Nearest Neighbors: Summary<br>In image classification we start with a training set of images and labels, and<br>must predict labels on the test set<br>The K-Nearest Neighbors classifier predicts labels based on the K nearest<br>training examples<br>Distance metric and K are hyperparameters<br>Choose hyperparameters using the validation set<br>Only run on the test set once at the very end!</p><p>小结：</p><blockquote><p>介绍了图像分类问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。<br>介绍了一个简单的图像分类器：最近邻分类器(Nearest Neighbor classifier)。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。<br>选取超参数的正确方法是：将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个。<br>如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。<br>一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。<br>最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。<br>最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。<br>在接下来的课程中，我们将专注于解决这些问题和挑战，并最终能够得到超过90%准确率的解决方案。该方案能够在完成学习就丢掉训练集，并在一毫秒之内就完成一张图片的分类</p></blockquote><h1 id="Linear-Classifier"><a href="#Linear-Classifier" class="headerlink" title="Linear Classifier"></a>Linear Classifier</h1><p>f(x,W) &#x3D; Wx + b<br>计算根据w，b选择分数最高的，即为该标签（w是向量）<br>1.代数表示<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20162739.png" alt="Alt text"></p><p>2.可视化表示<br>即用最合适得参数W和b改为矩阵表示<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20162739.png" alt="Alt text"><br>3.线性表示<br><img src="/img/cs231n/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-24%20162759.png" alt="Alt text"></p><p>参考文章：<br><a href="https://iphysresearch.github.io/blog/tag/cs231n/">https://iphysresearch.github.io/blog/tag/cs231n/</a></p>]]></content>
    
    
    <categories>
      
      <category>deeplearning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs231n</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SomeOne_Like_You</title>
    <link href="/2024/0188e3d28e.html"/>
    <url>/2024/0188e3d28e.html</url>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=16435049&auto=1&height=66"></iframe><p>···<br>                            Nothing compares， 无与伦比<br>                            No worries or cares， 无需担心和伤心<br>                            Regrets and mistakes， 遗憾与误解<br>                            They’re memories made， 在他们的回忆里<br>                            Who would have known， 有谁又能知晓<br>                            How bittersweet， 这其中的<br>                            This would taste， 酸甜苦涩</p><p><img src="/img/SomeOne_Like_You/1.png"></p>]]></content>
    
    
    <categories>
      
      <category>share_emotion</category>
      
    </categories>
    
    
    <tags>
      
      <tag>歌词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的26岁女房客</title>
    <link href="/2023/09430c6400.html"/>
    <url>/2023/09430c6400.html</url>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1335548766&auto=1&height=66"></iframe><p>先看看一个哈哈哈介绍视频;</p><iframe src="//player.bilibili.com/player.html?aid=321188303&bvid=BV16w411e72h&cid=1273443450&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><a href="https://www.bilibili.com/video/BV16w411e72h/?share_source=copy_web&vd_source=783046dd26b6d8ed3ae12d74958b0584">我的二十六岁女房客大型纪录片</a></p><p>这是一篇看了很久的小说，当然这也是一篇都市化爱情故事的网文，也许也有着大多数小说的爽文感觉。在没有读这篇小说时候，我心里其实对这个有些刻板印象，觉的可能就是’狗血剧情’，带着常见的套路，主角们分分合合的爱情故事，就是带着通常的“鄙夷”的态度打消时光看的。看书名，觉得水平可能就那样，但是当看完以后，心情难以描述，很复杂。当然，整小说这是幻想的，里面的爱情还是很脱离现实的，笔者也不能接受这种“暧昧”</p><hr><h2 id="1-故事简述"><a href="#1-故事简述" class="headerlink" title="1.故事简述"></a><strong>1.故事简述</strong></h2><p>确实是爱情剧，主角昭阳与三位女性的三段故事为主线，顺便掺杂着昭阳盆友们的故事与人性的变化，包括着友情与利益</p><p><em>a.爱情</em></p><p>昭阳在苏州大学的初恋及是简薇，他们之间的故事如同每个小说种大学生活一样美好。昭阳家庭是中高层阶级，而简薇属于富家女儿，老爸是广告公司董事长。按照常规故事情节，确实他们之间有着阻拦，大学四年之间的爱情在被简薇家庭阻拦以后，简薇被迫然后迫于诸多原因与昭阳分手。笔者记忆很深的镜头就是那天，薇薇简被迫要去美国，在去机场的路上，昭阳跟着车子追，薇薇留着泪，用口红在车窗上面写着等待······后来两年以后回国，又与昭阳牵扯着以前的美好</p><p>第二位女主即是乐瑶，他们是在酒吧认识的，两个灰心丧气的人碰在一起，并且行房事啦，导致乐瑶怀孕然后乐瑶准备流产。这本是一个意料之外的事情，昭阳一直把乐瑶当成妹妹或者很好的朋友看待，但乐瑶却已经深深爱上了他，尽管很多时候都要伪装。乐瑶也是一位富豪的女儿，但是后来与爸爸关系割裂，改名为乐，原本是肖瑶，她出来”流浪”结识昭阳。乐瑶从小没有过多得到过关爱，她只是拥有物质，但从来没有得到过爱。</p><p>第三位就是我们的女房客米彩，当然其实她是房东。她更是拥有留学高学历以及家里显赫的集团，她是女强人也是一个温柔心思缜密的女孩，全文是挑不出她的缺点，简直一个完美人物，她最后与昭阳在了一起。当然她也是一个外表强硬的女人，但内心其实并没有那么强大。小时候爸爸出车祸过世，母亲早已经离婚去了美国，从小寄养在叔叔家里。她和昭阳的感情是一步一步的从争吵矛盾中培养出来的，也是一步步米彩的宽容培养出来的。</p><p><em>b.友情</em></p><p>昭阳大学时候有两个兄弟，一个是方圆，一个是向晨，其中跟方圆最为要好。后来在薇薇离开以后，与一同喜欢音乐的罗本成为好兄弟。昭阳视为好老姐的之一是大学时候的颜研，也即是方圆女朋友后来是妻子，妍妍也是薇薇的好闺蜜，他们大学时候一起过着好玩的生活。另一好姐姐就是同样喜欢摇滚吉他的CC，CC也喜欢着罗本。总之昭阳的朋友分为两批，一是大学时候的，而是荒废时候乐队酒吧认识的。</p><p><em>c.地点</em></p><p>昭阳家在徐州，大学是在苏州大学，后来也是在苏州打拼，苏州旁边有一个西塘古镇，那是他们主角的散心地方。另外旁边的上海和南京是昭阳和米彩发展情节的地点</p><p><em>d.性格</em></p><p>昭阳;有点屌丝、说话多少喜欢带点不文明、性格直爽、耿直、有担当、痞帅、有点弹吉他的文艺、有做策划业务的才华</p><p>简薇：外表强硬、内心脆弱、女强人、执拗认真、不服输、硬撑</p><p>乐瑶：活泼可爱、有点坏“机灵”、有点女屌丝、伪装不在意</p><p>米彩：女强人、内心温柔、谨慎做事很稳、井井有条</p><p>李小允：朴实真诚、很适合贤惠、普通又很出众</p><hr><h2 id="2-不足与狗血e"><a href="#2-不足与狗血e" class="headerlink" title="2.不足与狗血e"></a><strong>2.不足与狗血e</strong></h2><p><em>A.暧昧太多</em></p><p>本来没有那么多误会的，可是主角跟每一段感情都藕断丝连，导致矛盾重重，大家都很难受，相信看过的都有这种感觉</p><p>（除非男主想一夫三妻）</p><p><em>B.巧合有时候太多</em></p><p>我知道小说巧合是很多但是有些时候，诺达的一座城说碰见就碰见了哈哈哈还有很多情节</p><hr><h2 id="3-感触"><a href="#3-感触" class="headerlink" title="3.感触"></a><strong>3.感触</strong></h2><p>就一句，真的就一句，就是书中的“人性的背后是白云苍狗，希望你我都做生活的高手”</p><p>里面的爱情还是幻想，笔者也不苟同这种方式</p><p>其他的难以描述很复杂，难以下笔</p><p>下面是引用：</p><blockquote><p>“”全文用昭阳的故事串起来了三位女主。<br>我最意难平的还是简薇。<br>我永远都忘不了，昭阳要退出公司时，简薇撕心裂肺的告诉昭阳，这是要了她的命啊，她在乎是钱吗，她在乎的真的只是昭阳啊，在乎那个昭阳曾许诺给她的空中之城。她一次次的在护城河边等着昭阳，也在承受不住一次次的打击之后告诉昭阳：<br>“这个世界好不公平，我陪你一路走过了<br>最低迷的时光，看过无数次你抱着吉他撕心裂肺的样子，可是几年后出现的米彩却轻易的夺走了现在这个最好的你，未来会更好的你……爱情真的要这么毫无道理吗？<br>“昭阳，我简薇永远是世上最你的女人，<br>只是爱错了时间，用错了方式。”<br>这个可以陪着昭阳把自己浇满汽油，陪着昭阳不要命的女人，是我最大的意难平。<br>而乐瑶更像人生中的导师，在昭阳一次次失败中总是点醒昭阳，给予昭阳帮助。<br>相对于简薇，乐瑶勇敢很多，她总是直接告诉昭阳：<br>“昭阳，忘了简薇吧，等我成为大明星只爱你一个”<br>“虽然路有点偏，有点难走，但你一定会走回那问老屋子里的，因为你心里有不能磨灭的信念。”<br>说起来乐瑶或许真的希望昭阳好过，哪怕在别人的世界里。因为乐瑶可以接受昭阳爱上米彩，唯独接受不了那个让昭阳陷入黑暗好几年的简薇，因为只有她清楚那时候的昭阳是怎么样的浑浑噩噩……<br>文中的第一女主，也是书名26岁的女房客，最好的米彩，她只用告诉昭阳一句话：<br>“前半生我为父亲而活，后半生我只为你而活”<br>而昭阳，我想了一遍又一遍，也只能说这么一句话了：<br>“他是昭阳，却没法做每个人的朝阳”<br>全书在跌宕起伏的情节中不断升华人物的特性，而方圆的故事，最为出色，也更验证了全文的核心主旨：<br>“人性的背后都是白云苍狗 愿你我能成为生活的高手”<br>护城河-大河东去不复返 简薇<br>火车轨-永远平行不相交 乐瑶<br>广场-是陪伴是守候 米彩<br>End。””</p></blockquote><blockquote><p>“”简微证明了，如果有误会就要直接挑明，而不是互相猜忌；乐瑶证明了，即使你走了99步，如果对面的人不肯向前，那将毫无意义；李小允证明了，在一个男人没有做好准备从头开始时，不要闯入，受伤的只会是自己；米彩证明了，爱情是真的存在的，只是需要克服重重困难，不放弃才能走到最后<br>乐瑶：俏皮可爱，心思细腻简微：强势，敢爱敢恨，嘴硬米彩：集合两人之长，避了两人之短米彩拿到了和简微几乎一样的试卷，不同的是她答了满分乐瑶，昭阳没有给她发试卷世人皆爱米彩，而我独爱乐瑶<br>人性的背后是白云苍狗，愿你我都能做生活的高手。<br>简微会因为昭阳的任性忍受痛苦，却不会找到昭阳沟通解决，乐瑶会因为昭阳的任性无怨无悔的陪伴，却不会带他走出困境与迷雾。而米彩会一次次改变自己，同时改变昭阳，让两个人物质与精神的世界越来越近。简微的爱是护城河边的烟酒，乐瑶的爱是酒吧里的放纵，米彩的爱是老屋子里拿不完的衣服，是坐坏的木马，是广场的赛车，是对昭阳过去的原谅，是一次次迁就昭阳失去卓美的无怨无悔，是甘心安心徐州做平凡女人的落差，是从爱的箴言到私奔。她其实在爱情中同样自私且敏感，缺乏安全感，却有相信昭阳一切的勇气，米彩凭什么输<br>乐瑶啊乐瑶，你真的成了大明星。故事开头那个挤在昭阳身边的娇小的身影，永远埋葬在孤岛上。乐瑶啊乐瑶，你真的见不得他受委屈，见不得他受伤，只要他需要帮助，你就出现在他身边。乐瑶啊乐瑶，结婚后过得好吗？还会去铁轨边看火车吗？在夜深人静的时候会想他吗？ 北京太大了，处处都是孤独的味道。逍遥池和今非谷，你真的喜欢吗<br>我看不到天空城，看不到苏州的江南水桥，看不到西塘的静冷河水，更看不到她穿着婚纱看着的是我的眼睛，好像结局我们都带着愧欠，当四目相对，分别的机会好像都不会再有了，结束了却好像又看见你在拉着我的手说这次永远不会分开了<br>“至此之后，我没有再和简薇有过任何形式的联系，她就像一颗绚烂的流星划过了我的生命里，一瞬的风景之后，便消失的悄无声息，偶尔从朋友那听说她过得还算不错，我几乎快要忘了她的模样，只是有时会在感性的夜里，突然想起她的声音，她说：要我好好生活。这是我生平听过的一句最为质朴的临别赠言，这似乎不足以概括我们之间经历过的一切，却又是人生最好方式的诠释，从此别过，纠缠半生，终成过客”<br>爱情到底是什么，又产生于何处呢?我有些想不透，或许所谓的爱情只是一只彩色的蝴蝶，看起来美丽却永远也能不接近，倘若你真想把她攥紧在手里，她便会挣扎，然后在挣扎中摩擦掉了所有色彩，从此苍白。——超级大坦克科比<br>我叫简薇，是他的初恋，我们有大学四年的恋爱时光，两年私奔的同甘共苦，我曾笃定我们会在一起一辈子，后来他爱上了一个完美无暇的女人，他们经历了我们经历过的一切，为什么他们就可以走到最后呢，我输的很不甘心，因为我爱他的一切，爱的灵魂，爱他的信仰，爱他描绘天空之城。<br>脚链还了，口红掉进河，吉他坏了，清空的笔记本，退股路酷，西塘也改了，护城河重建了，“昭阳”也只存在于清晨了，唯一留下的只有回忆“如果有下辈子，我一定不会让你看到我的心机，我要把全部的温柔都给你……这才是爱一个人的正确方式，可是，我明白的太晚了！”<br>旧城以西续前缘，莫愁一曲定终身<br>在欲望的城市，你就是我唯一的信仰。<br>昭阳长大吧赶紧长大””</p></blockquote><p>总之，男生可以读读消磨时间。而女生，是不大喜欢这情节的。</p>]]></content>
    
    
    <categories>
      
      <category>share_emotion</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>军爷留口饭吧</title>
    <link href="/2023/095b8f53d7.html"/>
    <url>/2023/095b8f53d7.html</url>
    
    <content type="html"><![CDATA[<p>军爷你们吃饭也太早了吧</p><p>我只有回来吃泡面了呜呜呜</p><p>(tmd气死我了！！！！)</p><p><img src="/img/%E5%86%9B%E7%88%B7%E7%95%99%E5%8F%A3%E9%A5%AD%E5%90%A7/a.jpg"></p>]]></content>
    
    
    <categories>
      
      <category>share_emotion</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>读曾国藩传所感</title>
    <link href="/2023/08d39805f6.html"/>
    <url>/2023/08d39805f6.html</url>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=2024683332&auto=1&height=66"></iframe><p>最近碰巧读了读《曾国藩传》，其实也是很想早就读了，可是以前一直推脱导致就不了了之。这次便迅速看了看，</p><p>于是记下一点感想。</p><p>由于知识和见识的狭隘，曾国藩我对他的了解很局限和刻板，之前认为不大算是一个”好人“。</p><p>现在想想在那个有限的年代，能做到中国最后一位大儒，还是十分的厉害。以前觉得曾国藩是一个长辫子的 旧式。官员，比较保守，可事实上他是开创洋务运动的可谓第一人，于那个时代有诸多先进的思想。下面我想聊聊我</p><p>敬佩他的四个点。</p><ol><li><p><strong>善于自省</strong></p><p>他本是一个资质很平平的人，不算天赋和能言善辩，甚至在年轻的一段时间内很是平淡。可他善于总结和写日记，把自己的毛病写下来。今天怎么样，做事处理如何，有么有什么不得当的地方。日积月累，他变得更加的从容与和蔼，更加的有思想认知。而他日复一日的自省，这种坚持是非常厉害的。使他的性格更加饱满、充实。</p><p>2.<strong>培养人才</strong></p><p>我们直到实际上李鸿章、左宗棠都是他的学生，他培养许多有内容有思想的一批官员，并且让清政府送孩子西方留学，学西方技术，以后留学归来报销中国。他有一种大局观念，他的学生也都饱受他的熏陶，日后也成为了那时期的一批人物。他后来的学生占据了全中国各个地方，甚至他可以推翻清政府。</p><p>3.<strong>胸怀很大</strong></p><p>左宗棠算是天赋很高的，但是做官却不像他老师那么顺利，他始终觉得他的老师的功绩应该是他的。当他的学生左宗棠竟然算计他时，不给他感恩反而揭露他的短，他并没有那么小气。当然他也彻底生气了，但不想左宗棠一样每天跟部下说他的哪里哪里不好（他的部下很大一部分原先时曾国藩手下的，手下也受不了）</p><p>而曾国藩不跟其他人说露左宗棠的坏话，他是那种不背后说小话的人，他觉得左宗棠说去把，他自己不在乎。</p><p>当左宗棠西征时，需要帮助（如银子、粮草），曾国藩并不遏制他，也没有给他穿小鞋（实际上左宗棠觉得曾肯定要报复他）。这时候，左宗棠真正明白了什么才是真正的大儒与人物，真心佩服这个笨拙的老师。</p><p>4.<strong>创新</strong></p><p>我们都知道洋务运动第一人就是他，他开办了学习西方的工厂和技术。并且办翻译厂，学习西方的精髓，一大批的西方知识书籍传入了中华大地。像梁启超这样的许多人都是阅读西方这书籍成长起来的。</p><p>当然，曾国藩本人也有局限性，也并不是完美的。读完以后，其实我读的很快，因为书中的文言文和引用我都跳过了哈哈哈。如今，对人物的认识更加立体。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>share_emotion</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>读《平凡的世界》所感</title>
    <link href="/2023/08bc6faff1.html"/>
    <url>/2023/08bc6faff1.html</url>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1450673431&auto=1&height=66"></iframe>~~喜欢听歌的盆友可以边听这首纯音乐边看哈哈哈也可以暂停~~<p>​    </p><p>一开始并没有打算读读这本小说，因为不知道从什么时候开始我对这本书有个刻板印象，总觉得这本书有点“土”，甚至莫名觉得故事就普通的情节再常见不过了。但事实上，看完这部著作，我就被疯狂打脸。如同看一部电影、一部番一样，直到看完这三册小说时，心里总有种莫名的难受，难以用言语去描述，空落落的。现在我如同所有看过这本著作的人一样，向大家推荐这本如土地般厚重的书籍。</p><p>​    我是随笔写写，写到哪里就是哪里，不会再拘束于高中语文一般分析这段象征了什么、这又是什么社会变革、又是什么多重表达，又运用了哪些手法，我是不愿意写成一篇阅读理解的，我只想写写自己的触动与思考。如果你还有闲时间，并且感兴趣，那就看下去吧。</p><p>故事发生于多个人物、多个地点上，并且随着人物的变化，地点也在变化。那我就先谈谈几个典型人物。在双水村，孙玉厚家里有四个孩子。大姐孙兰花，朴实人，却被那街溜子王满银花言巧语骗去了。由于生活在农村的局限性，大姐并没有得到过多的关爱，所以当这个王满银一顿海誓山盟甜言蜜语后，大姐就铁定了跟着他。王满银一年在外溜达，没留住村子干活，在外面的各个城市里如同乞讨一般生存，家里的兰花和两个孩子他似乎早已忘记，甚至一年下来可能过年也不回来，家里全靠兰花一个人务农拉扯两个孩子，王满银真真实实缺少一种品质，那就是责任与担当。而我们的老二即大哥孙少安就是一个责任的化身，才把初中勉强读完，13岁时候义无反顾回到家干活。因为一家人庞大，而父母也已经年老，家里需要一个顶梁柱，需要他照顾弟弟妹妹，他可以不上学了，但弟弟妹妹得一定得读书，孙家需要有人出人头地。老三即二哥孙少平，也是一个责任担当得男孩子，他读完高中以后没有机会上大学，最后出去揽工了（后面慢慢叙述这个主要人物）。老四即最小的妹妹孙兰香，是一个乖巧懂事、成绩斐然的女孩，也是家里保护的最好的一个孩子，最后她读了当地最好大学并且命运也十分幸运。</p><p>​    好啦，简单介绍了主角一家，就得介绍其他重要人物了。双水村村长田福堂，他的弟弟田福军在县上当二把手（以后升为地区一把手，最后还升到了省城任职）。田福堂有个女儿叫田润叶，田福军也有个女儿叫田晓霞。这对姐妹后来竟然爱上孙家的这一对兄弟（真是很巧，不过也算命中注定吧）。另外孙少平的发小叫金波，他俩一起上学、关系很好。金波的妹妹叫金秀，跟少平的妹妹兰香是好姐妹，她两一起上学。至于村中其他人物，我也不想再赘述了，因为实在太多了哈哈哈。接下来我主要分析的是爱情这根线，估计大家也最感兴趣。（当然还有很多揭露显示、反映生活文化的，爱情只是一部分，但我不仔细讲述了）</p><p>​    别忙，再把地点介绍一下。主角们从小长大的村子叫双水村，双水村归石圪节镇管，再上面的就是原西县，其上就是黄原市。然后黄原其上就是省城了，省城旁边有一个铜城市，铜城市专门产煤的。（这就是少平以后当工人的地方）。</p><p>​    首先谈谈孙少安和田润叶的事。他们俩从小一起长大，简直就是青梅竹马，润叶妹和少安哥。从小一起玩，一起上学，在双水村一起上小学时候，少安由于家贫穿的裤子都是补丁。某天屁股后面裤子裂了一个口子，大家都嘲笑他，他自己也很自卑就躲起来哭了。然后润叶偷偷找到了他，给他讲她帮他缝。少安脸红害羞，不情愿，然后<br>润叶直接叫他转身就给他缝起来了（哈哈哈虽然不过几天又坏了）。像这样的小事情很多，润叶从小就喜欢少安哥的担当，而起初少安也只是把润叶当成妹妹，并没有多想。但后来，在石圪节镇子读完初中以后，少安回家帮忙务农，润叶去县里读高中。两个人的世界从此便分离了，但润叶还是一如既往喜欢孙少安。尽管孙少安是个农民，润叶在县城教书，但她自己觉得没什么。大不了两人在一起后，就回双水村教书。到后来，孙少平在县城读高中时候，田润叶就经常叫少平带消息回去，给少平钱帮助他。给他粮票（知道他们家条件不好），对少平很好，少平大概也看出了什么。到后来随着进展，润叶把少安约出来，并递给了他情书。当少安看到表白以后，睁大了眼睛不敢相信。他刚开始很喜悦，他也确确实实喜欢润叶，但不久他就开始苦恼起来。他知道他们门不当户不对，最后肯定不会在一起，人家也不会把女儿嫁给他，这也就导致了以后的矛盾。润叶此时也在遭受二爸二妈家的介绍对象，并且二爸同事（此时是上司）的儿子对她一见钟情，并且穷追猛打。这官家子弟虽然没有什么文化，但人并不坏（他以后也因此承担了悲惨的一方）。润叶不喜欢他，甚至还反感，可是也不好直接会拒绝，因为这是二爸的同事家。<br>润叶如今两面为难，但孙少安自从接了那封信以后就逃避，连润叶鼓起勇气想找他挑明整个事情，他也是选择逃避“玩失踪”。这也就是全文中孙少安唯一营造的缺点。半年以后孙少安就娶了农村媳妇秀莲，但之前一直没有回复润叶的那封情书，他以为她会选择官家子弟会忘记他的，甚至还清润叶来他结婚坐席。（这简直了，但润叶肯定不会来的）最后润叶迫于压力，并且想给二爸官场帮助顺利点，便一时头昏和李向前（富家子弟）形式上结婚了。但婚后两人没有一起睡过觉，甚至形同陌生人，这给李向前带来了极大的痛苦少安这边婚姻还算幸福，秀莲也很勤劳，没有要孙家彩礼，并且勤勤恳恳帮助他们一家干活做饭，孙少安觉得能娶到秀莲也是很幸运的，只是偶尔想起了那润叶和她的表白信，但他马上又回归现实，他知道现实是他们两个不可能在一起。作为读者，有时候我很少安为什么选择逃避，不勇敢一点，起码要给润叶一个明确的回复或者解释。让别人干等着，最后自己结了婚。但我只是第三视角，我不能强加愿望，因为如果当这样的事情发生在我身上，我未必做的比少安好。对呀，就像如今我一个人，无论是晚饭吃馒头还是吃牛排，对我来说，并没有本质上的区别（硬说区别，就是牛排好吃点）。但当我深深爱上一个可爱的姑娘的时候，贫穷带给我的是深深的自卑，没有勇气去面对。现实的世界里，需要物质的保障，如果不能让你心爱的人过上美好的生活，跟自己在一起只有吃苦的话，那真是一道世界难题去抉择。</p><p><img src="/img/%E8%AF%BB%E3%80%8A%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C%E3%80%8B%E6%89%80%E6%84%9F/remark4.png"></p><h1 id="下面为少安润叶两人之间的原文描写句子"><a href="#下面为少安润叶两人之间的原文描写句子" class="headerlink" title="下面为少安润叶两人之间的原文描写句子"></a>下面为少安润叶两人之间的原文描写句子</h1><p>“他莫名其妙地把信从信封里抽出来，看见张纸上只写着两句话一<br>少安哥:<br>我愿意一辈子和你好。咱们慢慢再说这事。润叶。<br>孙少安站在公路上，一下子惊呆了他扭过头来，看见润叶已经穿过东拉河对的石坨节街道，消失在了供销门市部的后面街道后边的土山上空，一行南来的大雁正排成“人”字形，嗷嗷地欢叫着飞向了北方····”</p><p>“可是，他立刻就想到了润叶。尽管他对她<br>早已死了心，或者说根本就没有考虑过他和她<br>结合的可能性，但一旦他自己要找另外一个女<br>人的时候，他就以无比痛苦的心情又想到了润<br>叶。他伤心地认识到，他是多么地热爱和留恋<br>她。是的，他和她的感情本来就像苹果树上完<br>整的一枝，在那上面可以结出同样美丽的、红<br>脸蛋似的苹果来；现在却要把自己的那一部分<br>从上面剪下来，嫁接到另一棵不相同的树上<br>一天知道那会结出什么样的果实来。生活的<br>大剪刀是多么的无情，它要按照自己的安排来<br>对每一个人的命运进行剪裁！”</p><h2 id="下文为孙少平和田晓霞"><a href="#下文为孙少平和田晓霞" class="headerlink" title="下文为孙少平和田晓霞"></a>下文为孙少平和田晓霞</h2><p>当然除了少安和润叶，还有孙少平和田晓霞。他们之间的爱情似乎更让笔者动容。他们之间的故事更多，他和她虽只是隔壁班同学，但他们由于各个的超越同龄的思考和见解，互相吸引了对方。也在田晓霞的引导下，少平看了更多的书和报纸，少平的精神也在升华。可以说是，田晓霞是孙少平的一位贵人。他们之间的爱情更为动容，<br>而少平虽然也有哥哥少安一样的忧虑——身份地位不合适门不当户不对，但他更加的勇敢，他敢于去抓住自己幸福，即使最终结果不如所愿，但他仍感谢晓霞的出现。</p><p>少平经历了从教书老师变成城边的揽工，看起来和所有揽工一样邋遢，但他和所有人都不一样，他的精神更加强大，闪耀着光辉。他那肩膀后背全是伤痕和溃烂的肉，在夜晚悄悄脱离人群一个人点蜡读书，为烧菜的少女解围并拿出了打工所挣的钱给她。他可以在粗话漫天的工地和邋遢的人群中生活，也可以和晓霞一起高谈阔论看电影。<br>他的精神是充沛的，他敢于重头再来。他已经成长为一个真正的男人了，并且散发着男人那股坚强的毅力魅力。</p><p>“少平，你要记得，你与其他人不一样，你是一个有另外世界的人，你的心不应该只在这，而是在远方，那个充光的地方”这是田晓霞对如今身处工地底层少平的鼓舞。</p><p><img src="/img/%E8%AF%BB%E3%80%8A%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C%E3%80%8B%E6%89%80%E6%84%9F/remark5.png"></p><p>“直到现在，少平还难以相信今天发生了这<br>样的事！<br>他第一次拥抱了一个姑娘，并且亲吻了她<br>他饱饮了爱的甘露。他的青春出现了云霞般绚<br>丽的光彩。他真切地感受到了什么是幸福。幸<br>福！从此以后，他不管他处于什么样的境地，<br>他都可以自豪地说：我没有白白在这人世间枉<br>活一场！<br>他时而急匆匆地走着，时而又放慢脚步，<br>让那颗欢蹦乱跳的心稍许平静一些。前面不远<br>处就是大街，那里人声沸腾，一片纷扰。人<br>们你们知道吗？知道这城市有个揽工汉和地<br>委书记的女儿恋爱吗？你们也许没人会相信”</p><p><img src="/img/%E8%AF%BB%E3%80%8A%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C%E3%80%8B%E6%89%80%E6%84%9F/remark3.png"></p><p>“现在，他们在黑暗中踏着铁轨的枕木，肩<br>并肩相跟着向矿部那里走去。远处，灯火组成<br>了一个烂漫的世界。夜晚的矿区看起来无比的<br>壮丽。晓霞挽着他的胳膊，依偎着他，激动地<br>望着这个陌生的天地。初夏温暖的夜风轻轻吹<br>拂着这对幸福的青年。在黑户区的某个地方传<br>来轻柔的小提琴声，旋律竞是《如歌的行板》。<br>这里呀！并不是想象中的一片荒凉和粗莽；在<br>这远离都市的黑色世界里，到处漫流着生活的<br>温馨…<br>晓霞依偎着他，嘴里不由轻声哼起了《格<br>兰特船长和他的孩子们》中的那支插曲。少平<br>雄浑的男中音加入了进来，使那浪花飞溅的溪<br>流变成了波涛起伏的大河。唱吧，多好的夜<br>晚；即便没有月亮，心中也是一片皎洁！”</p><p>“孙少平倒在自己的床铺上，却怎么也睡不<br>着。几天来，他一直沉浸在一种异常的激动之<br>中，因为再过几天，就到了晓霞和他约定的那<br>个充满浪漫意味的日子。他们将在黄原古塔山<br>后面那棵杜梨树下相会，以不负他们两年前那<br>地方定下的爱的契约。呀！什么样的人生幸福<br>能比得上如此美炒的时刻？年轻的朋友、只有<br>你们大有这样的激情和想象力”</p><p>可是作者并不给一个美满结局。晓霞作为一名记者，在一次洪水报道中为了救小女孩而被洪水带走了生命。两人的浪漫约定的纪念日只有少平一个人如实赴约了。<br>晓霞不是那种温柔的女孩子，她是好动的 善辩的，像男孩子一样勇敢，但却在孙少平面前是一个娇羞的小女孩。</p><p>！<a href="/img/%E8%AF%BB%E3%80%8A%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C%E3%80%8B%E6%89%80%E6%84%9F/remark2.jpg">不要见外，不要见怪。田。</a></p><p>至于书中的人权 、人文的描写我就不讲述了，在这个以人为资源的中国，社会就是如你所想。<br>孙少平 孙少安，都是丰满充沛的人，都是坚韧正直的人。</p><p>结局是开放的，建议大家去读读原著。</p><p>好了，感谢你能看到最后呀！如果有什么感想，可以在评论区留言呢</p>]]></content>
    
    
    <categories>
      
      <category>share_emotion</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读后感</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>markdown 语法</title>
    <link href="/2023/0747770.html"/>
    <url>/2023/0747770.html</url>
    
    <content type="html"><![CDATA[<h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><p>xxiii</p><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><p>嘟嘟嘟</p><p>呜呜呜</p><h1 id="又是一级标题"><a href="#又是一级标题" class="headerlink" title="又是一级标题"></a>又是一级标题</h1><p>右边目录看出来有层次了吧</p><h1 id="文本样式"><a href="#文本样式" class="headerlink" title="文本样式"></a>文本样式</h1><h2 id="加粗"><a href="#加粗" class="headerlink" title="加粗"></a>加粗</h2><h2 id="ctrl-b-就欧克"><a href="#ctrl-b-就欧克" class="headerlink" title="ctrl +b 就欧克"></a><strong>ctrl +b 就欧克</strong></h2><h2 id="斜体"><a href="#斜体" class="headerlink" title="斜体"></a>斜体</h2><p><em>ctrl +i</em></p><h2 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h2><p><del>就左右两边同时两个波浪号</del></p><h2 id="全部粗体和斜体"><a href="#全部粗体和斜体" class="headerlink" title="全部粗体和斜体"></a>全部粗体和斜体</h2><blockquote><p><em><strong>hhhh</strong></em></p><p>左右各***</p></blockquote><h2 id="下标"><a href="#下标" class="headerlink" title="下标"></a>下标</h2><p><sub>youshangbiaoma</sub></p><h2 id="上标"><a href="#上标" class="headerlink" title="上标"></a>上标</h2><p><sup>youshangbiaoma</sup></p><h2 id="引用代码"><a href="#引用代码" class="headerlink" title="引用代码"></a>引用代码</h2><p>‘fiama’</p><p>“hhh”</p><p>‘’’</p><p>wuwuuw </p><p>“‘</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><p><a href="https://shallowu.github.io/">my blog</a></p><p><a href="https://shallowu.github.io/">https://shallowu.github.io/</a></p><p><a href="https://www.bilibili.com/">b tv</a></p><p>-hhh</p><p>*jjj</p><p>+rrr</p><ul><li>hhh</li><li>xxx</li><li>ggg</li></ul><ol><li>hhh</li><li>hhh</li></ol><p><img src="C:\Users\86178\Desktop\blog\blog\source\img\lucy.png" alt="lucy"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Master</span></span><br><span class="hljs-class">&#123;</span><br>private:<br><span class="hljs-type">char</span> name[<span class="hljs-number">20</span>];<br><span class="hljs-type">char</span> sex;<br><span class="hljs-type">int</span> age;<br><span class="hljs-type">char</span> phone;<br><span class="hljs-type">char</span> email[<span class="hljs-number">20</span>];<br><span class="hljs-type">char</span> birthday[<span class="hljs-number">20</span>];<br><span class="hljs-type">int</span> countEmail;<br><span class="hljs-type">char</span> textMessage[<span class="hljs-number">500</span>];<br><span class="hljs-type">int</span> countPhone;<br><span class="hljs-type">char</span> emailMessage[<span class="hljs-number">500</span>];<br></code></pre></td></tr></table></figure><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">hhhhh</span><br></code></pre></td></tr></table></figure><p>​哈哈哈哈哈哈哈哈哈[^1]</p><p><code>hhhhh</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 踩踩踩踩踩踩踩踩踩踩踩踩踩踩踩踩踩踩从"><br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 哈哈哈哈哈哈哈哈哈哈"><br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 咩咩咩咩咩咩咩咩咩咩咩咩"><br></code></pre></td></tr></table></figure><blockquote></blockquote><p><code>红红火火恍恍惚惚</code></p><iframe src="//player.bilibili.com/player.html?aid=786255285&bvid=BV1S14y1Q7Z5&cid=1208159634&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=545146747&auto=1&height=66"></iframe><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=524148450&auto=0&height=66"></iframe><iframe src="//player.bilibili.com/player.html?aid=786255285&bvid=BV1S14y1Q7Z5&cid=1208159634&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
